# -*- coding: utf-8 -*-
"""AI Implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m3iUq4YEVhN31vJtwVNWZhVQiHDOfzva

# MARSAD AI System

## Previous Experiences:
1. implement MRCNN for street and empty spaces segmentation with small and large dataset after augmentation, it was giving poor performance.
street model after enlarge the dataset (AP:21) script: https://colab.research.google.com/drive/18LxZ9qNnKNAObEYofJO47-7jayqZm87L?usp=sharing . empty space model after enlarge the dataset (AP:9) script: https://colab.research.google.com/drive/1wOyQOzuvb_dJFkLLckAM59evPx0HHgHk?usp=sharing


## Current Experiences:
1. Direct empty space segmentation
2. Detect the streets then segment the empty spaces from the cropped images.
3. Rotate the image to get the optimal BB for the streets before the segmentation.

## 0: install and download the related components
"""

!pip install ultralytics

# Download the street dataset
!gdown --id 1NJX47_X2qwJf4u5kDQEaIPN75lHqcsN0
!unzip -q /content/street_detection_dataset.zip -d street_detection_Dataset

# Download the empty space dataset
!gdown --id 1pBkh843bFwwr_CoJNclSSdl5MnPzxtjc
!unzip -q /content/EmptySpace_Segmentation.v2i.yolov11 -d emptySpace_segmentation_Dataset

# Download the test dataset
!gdown --id 1SimoYHYu-4W0sVIVrRXIbha1r70WObi_
!unzip -q /content/test_set.zip -d test_Dataset

# street weights download
!gdown --id 1FgGeLiDC_20SzpUatnBSqR_Oxed-2zGZ

# segment weights download
!gdown --id 1dahIi4LsJvQvVPbYKHyuO1jlX_tPef3I

"""## 1: Direct empty space segmentation"""

import torch
import cv2
import numpy as np
import matplotlib.pyplot as plt
from ultralytics import YOLO
from ultralytics.utils import ops

# Load the segmentation model
segmentation_model = YOLO('/content/weights.pt')

# Load and preprocess the full image
image_path = '/content/test_Dataset/test_set/images/testImage_png.rf.92af4d03328db9fb41807c2d61c06b4d.jpg'
img = cv2.imread(image_path)
if img is None:
    raise ValueError("Could not load image from the provided path.")

# Convert from BGR (OpenCV default) to RGB
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

# Resize the image to the required input size (e.g., 640x640)
target_size = 640  # Ensure it's divisible by 32
img_resized = cv2.resize(img_rgb, (target_size, target_size))

# Run the segmentation model
with torch.no_grad():
    segmentation_results = segmentation_model(img)

# Get the first result
seg_result = segmentation_results[0]

# Extract the mask if it exists
if seg_result.masks is not None and len(seg_result.masks.data) > 0:
    mask = seg_result.masks.data[0].cpu().numpy()
    mask_binary = (mask > 0.5).astype(np.uint8) * 255  # Convert to binary mask
else:
    mask_binary = np.zeros((target_size, target_size), dtype=np.uint8)  # Empty mask

# Resize the mask back to the original image size
mask_binary = cv2.resize(mask_binary, (img_rgb.shape[1], img_rgb.shape[0]))

# Visualization
plt.figure(figsize=(12, 5))

# Show original image
#plt.subplot(1, 2, 1)
#plt.imshow(img_rgb)
#plt.title("Original Image")
#plt.axis('off')

plt.subplot(1, 3, 3)
overlay = img_rgb.copy()
overlay[mask_binary > 0]  = [255, 0, 0]  # Red overlay for empty space
plt.imshow(overlay)
plt.title("direct predict of the empty space")
plt.axis('off')



plt.show()

"""## 2: Detect the streets then segment the empty spaces from the cropped images

Note: the following three cells have the same steps, the diffrence just in the printed output.

### 2.1: Show cropped images VS segmentation masks
"""

import torch
import cv2
import numpy as np
import matplotlib.pyplot as plt
from ultralytics import YOLO

# --------------------------
# Custom Letterbox Function (With Padding Correction)
# --------------------------
def letterbox(img, new_shape=(640, 640), color=(114, 114, 114)):
    """Resize image while keeping aspect ratio and adding padding if needed."""
    shape = img.shape[:2]  # Current shape (height, width)
    ratio = min(new_shape[0] / shape[0], new_shape[1] / shape[1])  # Scale ratio
    new_unpad = (int(round(shape[1] * ratio)), int(round(shape[0] * ratio)))  # New width, height
    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # Padding
    dw /= 2  # Split padding equally left and right
    dh /= 2  # Split padding equally top and bottom

    # Resize image
    img_resized = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)

    # Add padding
    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))  # Padding rounding
    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))  # Padding rounding
    img_padded = cv2.copyMakeBorder(img_resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)

    return img_padded, dw, dh  # Return both padded image and padding values

# --------------------------
# 1. Load the Models
# --------------------------
detection_model = YOLO('/content/street_detection_weights.pt')  # YOLOv12 detection model
segmentation_model = YOLO('/content/weights.pt')  # YOLOv11 segmentation model

# --------------------------
# 2. Load and Preprocess the Image
# --------------------------
image_path = '/content/test_Dataset/test_set/images/testImage_png.rf.92af4d03328db9fb41807c2d61c06b4d.jpg'
img = cv2.imread(image_path)
if img is None:
    raise ValueError("Could not load image from the provided path.")

# Convert from BGR to RGB
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

# Run detection on the full image
results = detection_model(image_path)

# --------------------------
# 3. Extract Bounding Boxes
# --------------------------
detections = results[0].boxes.data
conf_threshold = 0.5
filtered_detections = detections[detections[:, 4] >= conf_threshold]  # Keep only high-confidence detections

# --------------------------
# 4. Process Each Detected Street
# --------------------------
for i, det in enumerate(filtered_detections):
    # Extract and convert coordinates to integers
    x1, y1, x2, y2, conf, class_id = det
    x1, y1, x2, y2 = int(x1.item()), int(y1.item()), int(x2.item()), int(y2.item())

    # Crop the detected street region
    crop_img = img_rgb[y1:y2, x1:x2]

    # --------------------------
    # 5. Correct Preprocessing for Segmentation
    # --------------------------
    # Apply letterbox padding to match the segmentation model's expected size
    crop_padded, dw, dh = letterbox(crop_img, new_shape=(640, 640))  # Using our custom function

    # Convert to tensor and normalize
    crop_tensor = torch.from_numpy(crop_padded).float() / 255.0
    crop_tensor = crop_tensor.permute(2, 0, 1).unsqueeze(0)  # Shape: (1, 3, 640, 640)

    # Run segmentation
    with torch.no_grad():
        segmentation_results = segmentation_model(crop_tensor)

    # Extract mask
    seg_result = segmentation_results[0]
    if seg_result.masks is not None and len(seg_result.masks.data) > 0:
        mask = seg_result.masks.data[0].cpu().numpy()
        mask_binary = (mask > 0.5).astype(np.uint8) * 255
    else:
        mask_binary = np.zeros((640, 640), dtype=np.uint8)

    # --------------------------
    # 6. Remove Letterbox Padding Before Resizing Mask
    # --------------------------
    dh, dw = int(dh), int(dw)  # Ensure integer padding values
    mask_binary = mask_binary[dh:640 - dh, dw:640 - dw]  # Crop padding

    # Resize the mask back to the original cropped image size
    mask_binary = cv2.resize(mask_binary, (crop_img.shape[1], crop_img.shape[0]), interpolation=cv2.INTER_NEAREST)

    # --------------------------
    # 7. Visualization
    # --------------------------
    plt.figure(figsize=(12, 5))

    # Display the cropped street image
    plt.subplot(1, 2, 1)
    plt.imshow(crop_img)
    plt.title(f"Detected Street {i+1}")
    plt.axis('off')

    # Overlay the segmentation mask on the cropped image
    plt.subplot(1, 2, 2)
    plt.imshow(crop_img)
    plt.imshow(mask_binary, alpha=0.5, cmap='gray')
    plt.title(f"Segmentation Overlay {i+1}")
    plt.axis('off')

    plt.show()

"""### 2.2: Show the padding images with their corresponding masks"""

import torch
import cv2
import numpy as np
from ultralytics import YOLO
import matplotlib.pyplot as plt
import random
from scipy.ndimage import label

def letterbox(img, new_shape=(640, 640), color=(114, 114, 114)):
    shape = img.shape[:2]
    ratio = min(new_shape[0] / shape[0], new_shape[1] / shape[1])
    new_unpad = (int(round(shape[1] * ratio)), int(round(shape[0] * ratio)))
    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]
    dw /= 2
    dh /= 2

    img_resized = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)
    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))
    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))
    img_padded = cv2.copyMakeBorder(img_resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)

    return img_padded, dw, dh

# Load models
detection_model = YOLO('/content/street_detection_weights.pt')
segmentation_model = YOLO('/content/weights.pt')

# Load image
image_path = '/content/test_Dataset/test_set/images/testImage_png.rf.92af4d03328db9fb41807c2d61c06b4d.jpg'
img = cv2.imread(image_path)
if img is None:
    raise ValueError("Could not load image from the provided path.")

img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
image_h, image_w = img_rgb.shape[:2]
full_labeled_mask = np.zeros((image_h, image_w), dtype=np.uint16)
final_image = np.zeros_like(img_rgb)
segment_id = 1

# Detect streets
results = detection_model(image_path)
detections = results[0].boxes.data
conf_threshold = 0.5
filtered_detections = detections[detections[:, 4] >= conf_threshold]

for idx, det in enumerate(filtered_detections):
    x1, y1, x2, y2, conf, class_id = det
    x1, y1, x2, y2 = int(x1.item()), int(y1.item()), int(x2.item()), int(y2.item())
    crop_img = img_rgb[y1:y2, x1:x2]
    crop_padded, dw, dh = letterbox(crop_img, new_shape=(640, 640))

    # âœ… VISUALIZE the padded image before segmentation
    plt.figure(figsize=(5, 5))
    plt.imshow(crop_padded.astype(np.uint8))
    plt.title(f"Segmentation Input - Crop {idx+1}")
    plt.axis('off')
    plt.show()

    crop_tensor = torch.from_numpy(crop_padded).float() / 255.0
    crop_tensor = crop_tensor.permute(2, 0, 1).unsqueeze(0)

    # Run segmentation
    with torch.no_grad():
        segmentation_results = segmentation_model(crop_tensor)


    seg_result = segmentation_results[0]

    # Log info
    print(f"\nðŸŸ© Detection {idx+1}")
    if seg_result.masks is not None and len(seg_result.masks.data) > 0:
        print(f" - Number of masks: {len(seg_result.masks.data)}")
        print(f" - Mask shape: {seg_result.masks.data[0].shape}")

        # Visualize raw mask
        raw_mask = seg_result.masks.data[0].cpu().numpy()
        plt.figure(figsize=(4, 4))
        plt.imshow(raw_mask, cmap='gray')
        plt.title(f"Raw Mask (Detection {idx+1})")
        plt.axis('off')
        plt.show()

        # Threshold lowered from 0.5 to 0.3
        mask_binary = (raw_mask > 0.3).astype(np.uint8)
    else:
        print(" - No mask returned.")
        mask_binary = np.zeros((640, 640), dtype=np.uint8)

    dh, dw = int(dh), int(dw)
    mask_binary = mask_binary[dh:640 - dh, dw:640 - dw]
    mask_binary = cv2.resize(mask_binary, (crop_img.shape[1], crop_img.shape[0]), interpolation=cv2.INTER_NEAREST)

    segment_mask = (mask_binary == 1)
    full_labeled_mask[y1:y2, x1:x2][segment_mask] = segment_id
    segment_id += 1

    final_image[y1:y2, x1:x2] = crop_img

"""### 2.3: Merge the Cropped images and the segmentation masks"""

import torch
import cv2
import numpy as np
from ultralytics import YOLO
import matplotlib.pyplot as plt
import random
from scipy.ndimage import label

# --------------------------
# Custom Letterbox Function (With Padding Correction)
# --------------------------
def letterbox(img, new_shape=(640, 640), color=(114, 114, 114)):
    shape = img.shape[:2]  # Current shape (height, width)
    ratio = min(new_shape[0] / shape[0], new_shape[1] / shape[1])
    new_unpad = (int(round(shape[1] * ratio)), int(round(shape[0] * ratio)))
    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]
    dw /= 2
    dh /= 2

    img_resized = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)

    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))
    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))
    img_padded = cv2.copyMakeBorder(img_resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)

    return img_padded, dw, dh

# --------------------------
# Load Models
# --------------------------
detection_model = YOLO('/content/street_detection_weights.pt')
segmentation_model = YOLO('/content/weights.pt')

# --------------------------
# Load Image
# --------------------------
image_path = '/content/test_Dataset/test_set/images/testImage_png.rf.92af4d03328db9fb41807c2d61c06b4d.jpg'
img = cv2.imread(image_path)
if img is None:
    raise ValueError("Could not load image from the provided path.")

img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
image_h, image_w = img_rgb.shape[:2]

# Initialize labeled mask with unique IDs
full_labeled_mask = np.zeros((image_h, image_w), dtype=np.uint16)
final_image = np.zeros_like(img_rgb)
segment_id = 1

# --------------------------
# Detect Streets
# --------------------------
results = detection_model(image_path)
detections = results[0].boxes.data
conf_threshold = 0.5
filtered_detections = detections[detections[:, 4] >= conf_threshold]

# --------------------------
# Process Each Detection
# --------------------------
for det in filtered_detections:
    x1, y1, x2, y2, conf, class_id = det
    x1, y1, x2, y2 = int(x1.item()), int(y1.item()), int(x2.item()), int(y2.item())

    crop_img = img_rgb[y1:y2, x1:x2]
    crop_padded, dw, dh = letterbox(crop_img, new_shape=(640, 640))

    crop_tensor = torch.from_numpy(crop_padded).float() / 255.0
    crop_tensor = crop_tensor.permute(2, 0, 1).unsqueeze(0)

    with torch.no_grad():
        segmentation_results = segmentation_model(crop_tensor)

    seg_result = segmentation_results[0]
    if seg_result.masks is not None and len(seg_result.masks.data) > 0:
        mask = seg_result.masks.data[0].cpu().numpy()
        mask_binary = (mask > 0.5).astype(np.uint8)
    else:
        mask_binary = np.zeros((640, 640), dtype=np.uint8)

    dh, dw = int(dh), int(dw)
    mask_binary = mask_binary[dh:640 - dh, dw:640 - dw]
    mask_binary = cv2.resize(mask_binary, (crop_img.shape[1], crop_img.shape[0]), interpolation=cv2.INTER_NEAREST)

    # Apply unique label to this segment before merging
    segment_mask = (mask_binary == 1)
    full_labeled_mask[y1:y2, x1:x2][segment_mask] = segment_id
    segment_id += 1

    final_image[y1:y2, x1:x2] = crop_img

# --------------------------
# Merge Connected Components
# --------------------------
merged_mask = (full_labeled_mask > 0).astype(np.uint8)
labeled_mask, num_features = label(merged_mask)

# --------------------------
# Generate Colored Mask with Clear Colors
# --------------------------
final_mask_color = img_rgb.copy()
predefined_colors = [
    (255, 0, 0),      # Red
    (0, 255, 0),      # Green
    (0, 0, 255),      # Blue
    (255, 255, 0),    # Yellow
    (255, 0, 255),    # Magenta
    (0, 255, 255),    # Cyan
    (255, 165, 0),    # Orange
    (128, 0, 128),    # Purple
    (0, 128, 128),    # Teal
    (128, 128, 0)     # Olive
]

for i in range(1, num_features + 1):
    segment_mask = (labeled_mask == i)
    color = predefined_colors[(i - 1) % len(predefined_colors)]
    for c in range(3):
        final_mask_color[:, :, c] = np.where(segment_mask, color[c], final_mask_color[:, :, c])

# --------------------------
# Visualize Final Results
# --------------------------
plt.figure(figsize=(20, 8))

plt.subplot(1, 3, 1)
plt.imshow(img_rgb)
plt.title("Original Image")
plt.axis('off')

plt.subplot(1, 3, 2)
plt.imshow(final_image)
plt.title("Final Concatenated Street Image")
plt.axis('off')

plt.subplot(1, 3, 3)
plt.imshow(final_mask_color)
plt.title("street detection + empty space segmentation")
plt.axis('off')

plt.tight_layout()
plt.show()

# --------------------------
# Print Number of Empty Space Segments
# --------------------------
print(f"Number of connected empty space segments: {num_features}")

"""### 2.4: Compare GT, direct seg, and detect + seg. Calc the metrics"""

# Re-inserting the visualization block into the full code

import os
import cv2
import torch
import numpy as np
import matplotlib.pyplot as plt
from glob import glob
from ultralytics import YOLO
from sklearn.metrics import (
    jaccard_score, f1_score, precision_score, recall_score,
    average_precision_score, precision_recall_curve, auc
)
from scipy.ndimage import label
import contextlib
import io

# =============================
# âœ… CONFIG
# =============================
image_dir = "/content/emptySpace_segmentation_Dataset/test/images"
label_dir = "/content/emptySpace_segmentation_Dataset/test/labels"
segmentation_model = YOLO('/content/weights.pt')
detection_model = YOLO('/content/street_detection_weights.pt')
segmentation_model.verbose = False
detection_model.verbose = False

# =============================
# âœ… FUNCTIONS
# =============================
def letterbox(img, new_shape=(640, 640), color=(114, 114, 114)):
    shape = img.shape[:2]
    ratio = min(new_shape[0] / shape[0], new_shape[1] / shape[1])
    new_unpad = (int(round(shape[1] * ratio)), int(round(shape[0] * ratio)))
    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]
    dw /= 2
    dh /= 2
    img_resized = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)
    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))
    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))
    img_padded = cv2.copyMakeBorder(img_resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)
    return img_padded, dw, dh

def load_yolo_segmentation_label(label_path, img_shape):
    h, w = img_shape[:2]
    mask = np.zeros((h, w), dtype=np.uint8)
    if not os.path.exists(label_path):
        return mask
    with open(label_path, "r") as f:
        for line in f:
            parts = line.strip().split()
            if int(parts[0]) != 0:
                continue
            points = np.array(parts[1:], dtype=np.float32).reshape(-1, 2)
            points[:, 0] *= w
            points[:, 1] *= h
            cv2.fillPoly(mask, [points.astype(np.int32)], 1)
    return mask

def compute_ap50(gt_mask, pred_prob_mask):
    gt_flat = gt_mask.flatten()
    pred_flat = pred_prob_mask.flatten()
    if np.sum(gt_flat) == 0:
        return 0.0
    precision, recall, _ = precision_recall_curve(gt_flat, pred_flat)
    return auc(recall, precision)

def compute_metrics(gt, binary_pred, prob_pred):
    gt_flat = gt.flatten()
    binary_flat = binary_pred.flatten()
    prob_flat = prob_pred.flatten()
    return {
        "iou": jaccard_score(gt_flat, binary_flat, zero_division=0),
        "dice": f1_score(gt_flat, binary_flat, zero_division=0),
        "precision": precision_score(gt_flat, binary_flat, zero_division=0),
        "recall": recall_score(gt_flat, binary_flat, zero_division=0),
        "ap": average_precision_score(gt_flat, prob_flat),
        "ap50": compute_ap50(gt, prob_pred)
    }

def compute_fp_fn(gt, pred):
    gt_flat = gt.flatten()
    pred_flat = pred.flatten()
    tp = np.sum((gt_flat == 1) & (pred_flat == 1))
    fp = np.sum((gt_flat == 0) & (pred_flat == 1))
    fn = np.sum((gt_flat == 1) & (pred_flat == 0))
    return {"fp": fp, "fn": fn, "tp": tp}

# =============================
# âœ… METRIC CONTAINERS
# =============================
metrics_direct = {"iou": [], "dice": [], "precision": [], "recall": [], "ap": [], "ap50": []}
metrics_pipeline = {"iou": [], "dice": [], "precision": [], "recall": [], "ap": [], "ap50": []}
fp_fn_direct_total = {"fp": [], "fn": [], "tp": []}
fp_fn_pipeline_total = {"fp": [], "fn": [], "tp": []}
image_paths = sorted(glob(os.path.join(image_dir, "*.jpg")))

# =============================
# âœ… EVALUATION LOOP
# =============================
for image_path in image_paths:
    image_name = os.path.basename(image_path)
    label_path = os.path.join(label_dir, image_name.replace(".jpg", ".txt"))
    img = cv2.imread(image_path)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    h, w = img.shape[:2]
    gt_mask = load_yolo_segmentation_label(label_path, img.shape)

    # --- Direct Segmentation
    with contextlib.redirect_stdout(io.StringIO()):
        result_direct = segmentation_model(img, verbose=False)[0]
    soft_mask_direct = np.zeros((h, w), dtype=np.float32)
    mask_direct = np.zeros((h, w), dtype=np.uint8)
    if result_direct.masks is not None and len(result_direct.masks.data) > 0:
        m = result_direct.masks.data[0].cpu().numpy()
        soft_mask_direct = cv2.resize(m, (w, h))
        mask_direct = (soft_mask_direct > 0.2).astype(np.uint8)

    # --- Detection + Segmentation
    full_labeled_mask = np.zeros((h, w), dtype=np.uint16)
    soft_full_mask = np.zeros((h, w), dtype=np.float32)
    segment_id = 1

    with contextlib.redirect_stdout(io.StringIO()):
        results = detection_model(image_path, verbose=False)
    detections = results[0].boxes.data
    filtered_detections = detections[detections[:, 4] >= 0.2]

    for det in filtered_detections:
        x1, y1, x2, y2, conf, class_id = map(int, det[:6])
        crop_img = img_rgb[y1:y2, x1:x2]
        if crop_img.shape[0] == 0 or crop_img.shape[1] == 0:
            continue
        crop_padded, dw, dh = letterbox(crop_img)
        crop_tensor = torch.from_numpy(crop_padded).float() / 255.0
        crop_tensor = crop_tensor.permute(2, 0, 1).unsqueeze(0)

        with torch.no_grad():
            with contextlib.redirect_stdout(io.StringIO()):
                seg_result = segmentation_model(crop_tensor, verbose=False)[0]

        if seg_result.masks is None or len(seg_result.masks.data) == 0:
            continue

        for m in seg_result.masks.data:
            raw_mask = m.cpu().numpy()
            dh, dw = int(dh), int(dw)
            h_slice = slice(max(0, dh), min(640, 640 - dh))
            w_slice = slice(max(0, dw), min(640, 640 - dw))
            raw_mask = raw_mask[h_slice, w_slice]
            if raw_mask.shape[0] == 0 or raw_mask.shape[1] == 0:
                continue
            soft_crop_resized = cv2.resize(raw_mask, (crop_img.shape[1], crop_img.shape[0]), interpolation=cv2.INTER_LINEAR)
            soft_full_mask[y1:y2, x1:x2] = np.maximum(soft_full_mask[y1:y2, x1:x2], soft_crop_resized)
            mask_binary = (soft_crop_resized > 0.2).astype(np.uint8)
            segment_mask = (mask_binary == 1)
            full_labeled_mask[y1:y2, x1:x2][segment_mask] = segment_id
            segment_id += 1

    merged_mask = (full_labeled_mask > 0).astype(np.uint8)
    labeled_mask, num_features = label(merged_mask)

    # --- Compute Metrics & FP/FN
    metrics_direct_result = compute_metrics(gt_mask, mask_direct, soft_mask_direct)
    metrics_pipeline_result = compute_metrics(gt_mask, merged_mask, soft_full_mask)
    fp_fn_direct = compute_fp_fn(gt_mask, mask_direct)
    fp_fn_pipeline = compute_fp_fn(gt_mask, merged_mask)

    for k in metrics_direct:
        metrics_direct[k].append(metrics_direct_result[k])
        metrics_pipeline[k].append(metrics_pipeline_result[k])
    for k in fp_fn_direct:
        fp_fn_direct_total[k].append(fp_fn_direct[k])
        fp_fn_pipeline_total[k].append(fp_fn_pipeline[k])

    # --- Combined Mask (Union of Direct + Pipeline)
    combined_soft_mask = np.maximum(soft_mask_direct, soft_full_mask)
    combined_binary_mask = ((combined_soft_mask > 0.2).astype(np.uint8))

    # --- Compute Combined Metrics
    metrics_combined_result = compute_metrics(gt_mask, combined_binary_mask, combined_soft_mask)
    fp_fn_combined = compute_fp_fn(gt_mask, combined_binary_mask)

    # Store combined results
    if 'combined' not in locals():
        combined = {"iou": [], "dice": [], "precision": [], "recall": [], "ap": [], "ap50": []}
        fp_fn_combined_total = {"fp": [], "fn": [], "tp": []}

    for k in combined:
        combined[k].append(metrics_combined_result[k])
    for k in fp_fn_combined_total:
        fp_fn_combined_total[k].append(fp_fn_combined[k])


    # ----------------------
    # Visualization
    # ----------------------
    predefined_colors = [
        (255, 0, 0), (0, 255, 0), (0, 0, 255),
        (255, 255, 0), (255, 0, 255), (0, 255, 255),
        (255, 165, 0), (128, 0, 128), (0, 128, 128), (128, 128, 0)
    ]
    mask_color = img_rgb.copy()
    for i in range(1, num_features + 1):
        segment_mask = (labeled_mask == i)
        color = predefined_colors[(i - 1) % len(predefined_colors)]
        for c in range(3):
            mask_color[:, :, c] = np.where(segment_mask, color[c], mask_color[:, :, c])

    plt.figure(figsize=(20, 5))
    plt.subplot(1, 4, 1)
    plt.imshow(img_rgb)
    plt.title("Original")
    plt.axis('off')

    plt.subplot(1, 4, 2)
    plt.imshow(gt_mask, cmap='gray')
    plt.title("Ground Truth")
    plt.axis('off')

    plt.subplot(1, 4, 3)
    plt.imshow(mask_direct, cmap='gray')
    plt.title("Direct Segmentation")
    plt.axis('off')

    plt.subplot(1, 4, 4)
    plt.imshow(mask_color)
    plt.title("Detection + Segmentation")
    plt.axis('off')

    plt.suptitle(image_name)
    plt.tight_layout()
    plt.show()

# =============================
# âœ… FINAL METRICS
# =============================
def print_metrics(title, metric_dict):
    print(f"\n{title}")
    for k, v in metric_dict.items():
        print(f"{k.upper():<10}: {np.mean(v):.4f}")

def print_fp_fn(title, d):
    print(f"\n{title}")
    for k, v in d.items():
        print(f"{k.upper():<10}: {int(np.sum(v))}")

print("\nðŸ“Š Average Performance Metrics:")
print_metrics("Direct Segmentation", metrics_direct)
print_fp_fn("Direct Segmentation", fp_fn_direct_total)

print_metrics("Detection + Segmentation", metrics_pipeline)
print_fp_fn("Detection + Segmentation", fp_fn_pipeline_total)

print_metrics("Combined (Direct + Pipeline)", combined)
print_fp_fn("Combined (Direct + Pipeline)", fp_fn_combined_total)

import os
import cv2
import torch
import numpy as np
import matplotlib.pyplot as plt
from glob import glob
from ultralytics import YOLO
from scipy.ndimage import label
import contextlib
import io

# CONFIG
image_dir = "/content/emptySpace_segmentation_Dataset/test/images"
label_dir = "/content/emptySpace_segmentation_Dataset/test/labels"
segmentation_model = YOLO('/content/weights.pt')
detection_model = YOLO('/content/street_detection_weights.pt')
segmentation_model.verbose = False
detection_model.verbose = False

def letterbox(img, new_shape=(640, 640), color=(114, 114, 114)):
    shape = img.shape[:2]
    ratio = min(new_shape[0] / shape[0], new_shape[1] / shape[1])
    new_unpad = (int(round(shape[1] * ratio)), int(round(shape[0] * ratio)))
    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]
    dw /= 2
    dh /= 2
    img_resized = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)
    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))
    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))
    img_padded = cv2.copyMakeBorder(img_resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)
    return img_padded, dw, dh

def load_yolo_segmentation_label(label_path, img_shape):
    h, w = img_shape[:2]
    mask = np.zeros((h, w), dtype=np.uint8)
    if not os.path.exists(label_path):
        return mask
    with open(label_path, "r") as f:
        for line in f:
            parts = line.strip().split()
            if int(parts[0]) != 0:
                continue
            points = np.array(parts[1:], dtype=np.float32).reshape(-1, 2)
            points[:, 0] *= w
            points[:, 1] *= h
            cv2.fillPoly(mask, [points.astype(np.int32)], 1)
    return mask

# PROCESS ONE EXAMPLE IMAGE
image_path = sorted(glob(os.path.join(image_dir, "*.jpg")))[6]
image_name = os.path.basename(image_path)
label_path = os.path.join(label_dir, image_name.replace(".jpg", ".txt"))
img = cv2.imread(image_path)
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
h, w = img.shape[:2]
gt_mask = load_yolo_segmentation_label(label_path, img.shape)

# DIRECT SEGMENTATION
with contextlib.redirect_stdout(io.StringIO()):
    result_direct = segmentation_model(img, verbose=False)[0]
soft_mask_direct = np.zeros((h, w), dtype=np.float32)
if result_direct.masks is not None and len(result_direct.masks.data) > 0:
    m = result_direct.masks.data[0].cpu().numpy()
    soft_mask_direct = cv2.resize(m, (w, h))

# DETECTION + SEGMENTATION
soft_full_mask = np.zeros((h, w), dtype=np.float32)
with contextlib.redirect_stdout(io.StringIO()):
    results = detection_model(image_path, verbose=False)
detections = results[0].boxes.data
filtered_detections = detections[detections[:, 4] >= 0.2]

for det in filtered_detections:
    x1, y1, x2, y2, conf, class_id = map(int, det[:6])
    crop_img = img_rgb[y1:y2, x1:x2]
    if crop_img.shape[0] == 0 or crop_img.shape[1] == 0:
        continue
    crop_padded, dw, dh = letterbox(crop_img)
    crop_tensor = torch.from_numpy(crop_padded).float() / 255.0
    crop_tensor = crop_tensor.permute(2, 0, 1).unsqueeze(0)

    with torch.no_grad():
        with contextlib.redirect_stdout(io.StringIO()):
            seg_result = segmentation_model(crop_tensor, verbose=False)[0]

    if seg_result.masks is None or len(seg_result.masks.data) == 0:
        continue

    for m in seg_result.masks.data:
        raw_mask = m.cpu().numpy()
        dh, dw = int(dh), int(dw)
        h_slice = slice(max(0, dh), min(640, 640 - dh))
        w_slice = slice(max(0, dw), min(640, 640 - dw))
        raw_mask = raw_mask[h_slice, w_slice]
        if raw_mask.shape[0] == 0 or raw_mask.shape[1] == 0:
            continue
        soft_crop_resized = cv2.resize(raw_mask, (crop_img.shape[1], crop_img.shape[0]), interpolation=cv2.INTER_LINEAR)
        soft_full_mask[y1:y2, x1:x2] = np.maximum(soft_full_mask[y1:y2, x1:x2], soft_crop_resized)

# FINAL COMBINED MASK
combined_soft_mask = np.maximum(soft_mask_direct, soft_full_mask)
combined_binary_mask = (combined_soft_mask > 0.2).astype(np.uint8)

# VISUALIZATION
plt.figure(figsize=(15, 5))
plt.subplot(1, 3, 1)
plt.imshow(img_rgb)
plt.title("Original Image")
plt.axis('off')

plt.subplot(1, 3, 2)
plt.imshow(gt_mask, cmap='gray')
plt.title("Ground Truth Mask")
plt.axis('off')

plt.subplot(1, 3, 3)
plt.imshow(combined_binary_mask, cmap='gray')
plt.title("Final Pipeline Mask")
plt.axis('off')

plt.suptitle(image_name)
plt.tight_layout()
plt.show()

"""## 3: Rotate the image to get the optimal BB for the streets before the segmentation

### 3.1: show the rotated images for each bounding box
"""

import cv2
import numpy as np
import torch
from ultralytics import YOLO
import matplotlib.pyplot as plt

# Load model
model = YOLO('/content/street_detection_weights.pt')

# Load original image
image_path = '/content/test_Dataset/test_set/images/testImage_png.rf.92af4d03328db9fb41807c2d61c06b4d.jpg'
original_img = cv2.imread(image_path)
original_img_rgb = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)
h, w = original_img.shape[:2]

# Detect original bounding boxes
original_results = model(original_img)
conf_threshold = 0.5
result = original_results[0]
detections = result.boxes.xyxy.cpu().numpy()
confidences = result.boxes.conf.cpu().numpy()

# Keep only high-confidence detections
original_boxes = detections[confidences >= conf_threshold]


# Rotate image
def rotate_image(image, angle):
    (h, w) = image.shape[:2]
    center = (w // 2, h // 2)
    matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
    cos, sin = np.abs(matrix[0, 0]), np.abs(matrix[0, 1])
    nW = int((h * sin) + (w * cos))
    nH = int((h * cos) + (w * sin))
    matrix[0, 2] += (nW / 2) - center[0]
    matrix[1, 2] += (nH / 2) - center[1]
    return cv2.warpAffine(image, matrix, (nW, nH)), matrix

# IoU
def compute_iou(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])
    interArea = max(0, xB - xA) * max(0, yB - yA)
    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])
    iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)
    return iou

# Track best rotations
best_crops = []

for idx, original_box in enumerate(original_boxes):
    best_area = (original_box[2] - original_box[0]) * (original_box[3] - original_box[1])
    best_angle = 0
    best_img = original_img.copy()
    best_box = original_box

    for angle in range(-20, 21):
        rotated_img, _ = rotate_image(original_img, angle)
        result = model(rotated_img)[0]
        new_boxes = result.boxes.xyxy.cpu().numpy() if result.boxes else []

        best_iou = 0
        matching_box = None
        for new_box in new_boxes:
            iou = compute_iou(original_box, new_box)
            if iou > best_iou:
                best_iou = iou
                matching_box = new_box

        if matching_box is not None:
            area = (matching_box[2] - matching_box[0]) * (matching_box[3] - matching_box[1])
            if area < best_area:
                best_area = area
                best_angle = angle
                best_img = rotated_img.copy()
                best_box = matching_box

    best_crops.append((idx + 1, best_img, best_box, best_angle, best_area))

# Plot
fig, axes = plt.subplots(1, len(best_crops), figsize=(20, 6))
for i, (crop_idx, img, box, angle, area) in enumerate(best_crops):
    axes[i].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    axes[i].add_patch(plt.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1],
                                    edgecolor='lime', facecolor='none', linewidth=2))
    axes[i].set_title(f"Crop {crop_idx}\nAngle: {angle}Â°\nArea: {int(area)}")
    axes[i].axis('off')
plt.tight_layout()
plt.show()

"""### 3.2: show the padding image with its segmentation mask"""

import torch
import cv2
import numpy as np
from ultralytics import YOLO
import matplotlib.pyplot as plt
import random
from scipy.ndimage import label

# Letterbox padding
def letterbox(img, new_shape=(640, 640), color=(114, 114, 114)):
    shape = img.shape[:2]
    ratio = min(new_shape[0] / shape[0], new_shape[1] / shape[1])
    new_unpad = (int(round(shape[1] * ratio)), int(round(shape[0] * ratio)))
    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]
    dw /= 2
    dh /= 2
    img_resized = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)
    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))
    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))
    img_padded = cv2.copyMakeBorder(img_resized, top, bottom, left, right,
                                    cv2.BORDER_CONSTANT, value=color)
    return img_padded, dw, dh

# Load models
detection_model = YOLO('/content/street_detection_weights.pt')
segmentation_model = YOLO('/content/weights.pt')

# Load image
image_path = '/content/test_Dataset/test_set/images/testImage_png.rf.92af4d03328db9fb41807c2d61c06b4d.jpg'
img = cv2.imread(image_path)
if img is None:
    raise ValueError("Could not load image from the provided path.")

img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
image_h, image_w = img_rgb.shape[:2]
full_labeled_mask = np.zeros((image_h, image_w), dtype=np.uint16)
final_image = np.zeros_like(img_rgb)
segment_id = 1

# Detect original bounding boxes
original_results = detection_model(original_img)
conf_threshold = 0.5
result = original_results[0]
detections = result.boxes.xyxy.cpu().numpy()
confidences = result.boxes.conf.cpu().numpy()
original_boxes = detections[confidences >= conf_threshold]

# Rotate helper
def rotate_image(image, angle):
    (h, w) = image.shape[:2]
    center = (w // 2, h // 2)
    matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
    cos, sin = np.abs(matrix[0, 0]), np.abs(matrix[0, 1])
    nW = int((h * sin) + (w * cos))
    nH = int((h * cos) + (w * sin))
    matrix[0, 2] += (nW / 2) - center[0]
    matrix[1, 2] += (nH / 2) - center[1]
    return cv2.warpAffine(image, matrix, (nW, nH)), matrix

# IoU
def compute_iou(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])
    interArea = max(0, xB - xA) * max(0, yB - yA)
    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])
    return interArea / float(boxAArea + boxBArea - interArea + 1e-6)



# Final padded crops and masks
final_results = []

for idx, original_box in enumerate(original_boxes):
    best_box = original_box
    best_area = (original_box[2] - original_box[0]) * (original_box[3] - original_box[1])
    best_img = original_img.copy()
    best_angle = 0

    for angle in range(-20, 21):
        rotated_img, _ = rotate_image(original_img, angle)
        result = detection_model(rotated_img)[0]
        new_boxes = result.boxes.xyxy.cpu().numpy() if result.boxes else []

        best_iou = 0
        match_box = None
        for box in new_boxes:
            iou = compute_iou(original_box, box)
            if iou > best_iou:
                best_iou = iou
                match_box = box

        if match_box is not None:
            area = (match_box[2] - match_box[0]) * (match_box[3] - match_box[1])
            if area < best_area:
                best_area = area
                best_img = rotated_img.copy()
                best_box = match_box
                best_angle = angle

    # Crop and pad
    x1, y1, x2, y2 = map(int, best_box)
    crop_img = best_img[y1:y2, x1:x2]
    padded_crop, dw, dh = letterbox(crop_img)

    tensor_input = torch.from_numpy(padded_crop).float() / 255.0
    tensor_input = tensor_input.permute(2, 0, 1).unsqueeze(0)

    with torch.no_grad():
        seg_result = segmentation_model(tensor_input)[0]

    if seg_result.masks is not None and len(seg_result.masks.data) > 0:
        mask = seg_result.masks.data[0].cpu().numpy()
        mask = cv2.resize(mask, (padded_crop.shape[1], padded_crop.shape[0]), interpolation=cv2.INTER_NEAREST)
    else:
        mask = np.zeros((padded_crop.shape[0], padded_crop.shape[1]))


    final_results.append((padded_crop, mask))

# Visualize padded images and masks
fig, axes = plt.subplots(len(final_results), 2, figsize=(10, 5 * len(final_results)))
for i, (padded_crop, mask) in enumerate(final_results):
    axes[i, 0].imshow(cv2.cvtColor(padded_crop, cv2.COLOR_BGR2RGB))
    axes[i, 0].set_title(f"Padded Image {i+1}")
    axes[i, 0].axis('off')

    axes[i, 1].imshow(mask, cmap='gray')
    axes[i, 1].set_title(f"Segmentation Mask {i+1}")
    axes[i, 1].axis('off')

plt.tight_layout()
plt.show()

import cv2
import numpy as np
import torch
from ultralytics import YOLO
import matplotlib.pyplot as plt
from scipy.ndimage import label

# ---------- Helper: Letterbox ----------
def letterbox(img, new_shape=(640, 640), color=(114, 114, 114)):
    shape = img.shape[:2]
    ratio = min(new_shape[0] / shape[0], new_shape[1] / shape[1])
    new_unpad = (int(round(shape[1] * ratio)), int(round(shape[0] * ratio)))
    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]
    dw /= 2
    dh /= 2
    img_resized = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)
    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))
    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))
    img_padded = cv2.copyMakeBorder(img_resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)
    return img_padded, dw, dh

# ---------- Helper: Rotate Image ----------
def rotate_image(image, angle, center=None, scale=1.0):
    (h, w) = image.shape[:2]
    if center is None:
        center = (w // 2, h // 2)
    matrix = cv2.getRotationMatrix2D(center, angle, scale)
    cos = np.abs(matrix[0, 0])
    sin = np.abs(matrix[0, 1])
    nW = int((h * sin) + (w * cos))
    nH = int((h * cos) + (w * sin))
    matrix[0, 2] += (nW / 2) - center[0]
    matrix[1, 2] += (nH / 2) - center[1]
    return cv2.warpAffine(image, matrix, (nW, nH)), matrix

# ---------- Helper: Inverse Rotate Point ----------
def inverse_rotate_mask(mask, matrix, output_shape):
    inverse_matrix = cv2.invertAffineTransform(matrix)
    return cv2.warpAffine(mask, inverse_matrix, output_shape, flags=cv2.INTER_NEAREST)

# ---------- Load Models ----------
detection_model = YOLO('/content/street_detection_weights.pt')
segmentation_model = YOLO('/content/weights.pt')

# ---------- Load Image ----------
image_path = '/content/test_Dataset/test_set/images/testImage_png.rf.92af4d03328db9fb41807c2d61c06b4d.jpg'
original_img = cv2.imread(image_path)
img_rgb = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)
image_h, image_w = img_rgb.shape[:2]
full_binary_mask = np.zeros((image_h, image_w), dtype=np.uint8)

# ---------- Detect Streets ----------
detections = detection_model(original_img)[0].boxes.data
conf_threshold = 0.5
filtered = detections[detections[:, 4] >= conf_threshold]

for det in filtered:
    x1, y1, x2, y2 = map(int, det[:4])
    best_angle = 0
    min_area = float('inf')
    best_crop = None
    best_matrix = None

    # Find best rotation to minimize bounding box
    for angle in range(-20, 21):
        rotated_img, matrix = rotate_image(original_img, angle)
        rotated_crop = rotated_img[y1:y2, x1:x2]
        if rotated_crop.size == 0:
            continue
        area = rotated_crop.shape[0] * rotated_crop.shape[1]
        if area < min_area:
            min_area = area
            best_angle = angle
            best_crop = rotated_crop
            best_matrix = matrix

    # Process best crop
    padded_crop, dw, dh = letterbox(best_crop)
    tensor_input = torch.from_numpy(padded_crop).float() / 255.0
    tensor_input = tensor_input.permute(2, 0, 1).unsqueeze(0)

    with torch.no_grad():
        result = segmentation_model(tensor_input)[0]

    if result.masks is not None and len(result.masks.data) > 0:
        for mask_tensor in result.masks.data:
            mask = mask_tensor.cpu().numpy()
            mask = cv2.resize(mask, (padded_crop.shape[1], padded_crop.shape[0]), interpolation=cv2.INTER_NEAREST)
            binary_mask = (mask > 0.5).astype(np.uint8)

            # Remove padding
            top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))
            left, right = int(round(dw - 0.1)), int(round(dw + 0.1))
            binary_mask = binary_mask[top:binary_mask.shape[0] - bottom, left:binary_mask.shape[1] - right]

            # Resize to original crop size
            binary_mask = cv2.resize(binary_mask, (best_crop.shape[1], best_crop.shape[0]), interpolation=cv2.INTER_NEAREST)

            # Rotate mask back
            mask_back = inverse_rotate_mask(binary_mask, best_matrix, (image_w, image_h))

            # Merge with full binary mask
            full_binary_mask = np.maximum(full_binary_mask, mask_back)

# ---------- Label Connected Components ----------
labeled_mask, num_features = label(full_binary_mask)
final_mask_color = img_rgb.copy()
colors = [
    (255, 0, 0), (0, 255, 0), (0, 0, 255),
    (255, 255, 0), (255, 0, 255), (0, 255, 255)
]

for i in range(1, num_features + 1):
    segment = (labeled_mask == i)
    color = colors[(i - 1) % len(colors)]
    for c in range(3):
        final_mask_color[:, :, c] = np.where(segment, color[c], final_mask_color[:, :, c])

# ---------- Plot ----------
plt.figure(figsize=(15, 7))
plt.subplot(1, 2, 1)
plt.imshow(img_rgb)
plt.title("Original Image")
plt.axis('off')

plt.subplot(1, 2, 2)
plt.imshow(final_mask_color)
plt.title("Final Segmentation Mask on Original Image")
plt.axis('off')
plt.tight_layout()
plt.show()

print(f"Number of connected empty space segments: {num_features}")

"""### 3.3: The observation:
1. This method takes more computing and time to found the optimal rotation angle for each bounding box (the test image takes 1m while method 2 takes 5s).
2. The segmentation result in this methode for some reason worst than the pre method.
"""